\chapter{Related work}

In this chapter we provide an overview of work in the fields from which we draw inspiration for our own work.
These fields are data organization, digital signal processing, dimensionality reduction, and explainable machine learning.
While there exists considerable overlap between the topics in these fields upon which this dissertation touches, we find the terminology does not always align.
To be able to draw the necessary parallels, we do not structure this chapter through the four aforementioned fields, but through the topics we encounter further in this dissertation.
These are relational database management systems, which readers with a statistical background may be unfamiliar with, and time series.

\section{Relational database management systems}

If we do not store a large data set in secondary storage, we cannot model it.
Statisticians commonly retrieve data from \acrlong{csv}~\acrshort{csv} \citep{DBLP:journals/rfc/rfc4180} or \acrlong{tsv}~(\acrshort{tsv}) \citep{tsv} files.
Being text-based file formats, \acrshort{csv} and \acrshort{tsv} enjoy the benefit of human readability in any text editor.
\acrshort{csv} files only go so far, vector-valued fields such as sensor waveform samples would make them too wide.
For optimizing search in \acrshort{csv} files, we depend on indexing, which not every file system or operating system implements.
\Acrlong{rdbms} (\acrshort{rdbms}) allow us to store data in and retrieve data from a relational data model \citep{DBLP:journals/cacm/Codd70}.
Aside from allowing us to store complex data types and optimizing search by indexing, an \acrshort{rdbms} protects the data from various anomalies which occur when we improperly update a collection of files.
Three topics are important in understanding the performance of a relational data model: keys, normalization, and indices.

\subsection{Keys}%

A superkey consists of a subset of a table's columns' values that uniquely identifies the remaining columns' values, i.e. all columns' values not part of the superkey are functionally dependent on it.
The term functionally dependent refers to the fact that we can construct a function which takes the superkey's columns' values as an argument and returns the remaining columns' values.
When multiple superkeys exist for a table, the superkeys that span the least amount of columns are the candidate keys for said table.
Candidate keys for a table may overlap in columns.
As a convention carried over from navigational DBMSs, we choose a single candidate key as the primary key for a table.
Other tables' rows reference rows in our table by including the columns its primary key spans.
We then call these columns the foreign key referencing another table.

When a superkey for a table arises from the domain of discourse, we refer to it as a natural key.
Not all tables have columns that uniquely identify them, however.
In addition, we want to update rows in a table without having to update the foreign keys for the tables that reference them, which means we cannot use natural keys as foreign keys.
We therefore artificially introduce a superkey for a table, which we call a surrogate key.
We note that the use of surrogate keys does not eliminate the need to identify natural keys and functional dependencies as a prerequisite for proper relational database design.

\subsection{Normalization}%

Codd conceived the relational model to allow the columns and rows of tables to evolve independently from the columns and rows of other tables~\citep{DBLP:journals/cacm/Codd70}.
To correctly use the relational model, we therefore split data into multiple tables to reduce data redundancy, a process we refer to as normalization.
Normalization is not just splitting data into tables haphazardly\thinspace: it is a formalized process akin to refactoring source code in a programming language.
To guide us through this process, there exists a hierarchy of normal forms defined by several academics throughout the years~\citep{DBLP:journals/cacm/Kent83}.
Each of these eliminates a type of data redundancy and the data integrity concerns such as insertion, update, and deletion anomalies that come with it.

\begin{itemize}
\item Tables in first normal form (1NF) have the same amount of columns in each of their rows. The astute reader of the second chapter of this document may note that this is always the case for relational data. Indeed, 1NF is a necessary and sufficient condition on data to enter the relational model. To achieve 1NF, we create tables for each different set of columns across rows of unnormalized data.
\item Tables in second normal form (2NF) are in 1NF and only have non-key columns which are functionally dependent on all columns of a candidate key. If at least one of the non-key columns is functionally dependent on only part of the columns of a candidate key, a table does not satisfy 2NF. These columns' values are then duplicated and must be updated in multiple rows. An update or deletion anomaly occurs when we fail to update or delete all of these rows at once. To achieve 2NF, we split the table into a table per part of the candidate key and include its functionally dependent columns in said table.
\item Tables in third normal form (3NF) are in 2NF and do not have non-key columns which are functionally dependent on other non-key columns. The update and deletion anomalies for data not in 3NF occur when we fail to update the functionally dependent non-key columns in all rows. They are therefore similar to the update anomalies we encounter for data not in 2NF. When we encounter non-key columns which identify other non-key columns, we split these off into their own tables to achieve 3NF.
\item Tables in 3NF are always in Boyce--Codd normal form (BCNF)~\citep{DBLP:conf/sigmod/Heath71}, unless they have multiple overlapping candidate keys. In that case, to be in BCNF, a table's functional dependencies must come from superkeys only, i.e. non-key columns cannot be the domain of a functional dependency. BCNF eliminates all data redundancy arising from functional dependencies.
\item Tables in fourth normal form (4NF) are in BCNF and do not express more than 1 many-to-many or one-to-many relationship, which are both special cases of multivalued dependencies. When they do, there are multiple ways of distributing the columns's values for those relationships across rows, which lead to insertion, update, and deletion anomalies.
\item Tables in fifth normal form (5NF) are in 4NF and cannot be reconstructed by joining several tables together. Formally, we say every non-trivial join dependency for tables in 5NF is implied by their candidate keys. A table \(T\) has a join dependency on a set of tables \(T_i\) when \(T = \Bowtie_{i=1}^{n} T_i\). We call a join dependency trivial when that set of tables is just \(\{ T \}\). The multivalued dependencies 4NF restricts are binary join dependencies.
\item Like 4NF and 5NF, domain-key normal form (DK/NF)~\citep{DBLP:journals/tods/Fagin81} is not defined in terms of functional dependencies. Tables are in DK/NF when all their constraints are the logical consequence of their keys and their columns' domains. A typical violation of DK/NF is the use of a foreign key which may reference multiple tables depending on another column's value. The data integrity of tables in DK/NF can be preserved by any RDBMS which supports foreign keys\thinspace: DK/NF is the lowest normal form which eliminates all data redundancy and associated insertion, update, and deletion anomalies.
\item Tables in sixth normal form (6NF) consist of a single candidate key and only one non-key column. In other words, the only join dependency these tables have is the trivial one. The advantage of 6NF over DK/NF is apparent for multidimensional data: it makes it possible to add columns as new tables, without modifying existing tables. In effect, it emulates part of a wide column store with a relational database. Tables in 6NF also trivially satisfy 5NF, but not necessarily DK/NF.
\end{itemize}

\subsection{Indices}%

Indices containing keys derived from the values for a subset of the columns of a table are used by \acrshortpl{rdbms} when locating rows.
In \acrshort{rdbms}, indices are an implementation detail\thinspace: applications do not depend on them to yield correct results.
\Acrshort{rdbms} can of course also locate rows without using an index by sequentially scanning tables.
Because this requires them to read the entire table from secondary storage, either as a whole or in pages, it is not the most performant approach.
We say a sequential table scan has a time complexity of \(O(n)\), where \(n\) is the amount of rows in the table.
It is not just the worst case analysis for a sequential scan that leads us to \(O(n)\), it is also the best and average case, as the RDBMS always reads every row from secondary storage to see whether it is one of the rows it is trying to locate.

\Acrshortpl{rdbms} use indices to do better than \(O(n)\) sequential table scans for locating rows.
How much better they do is determined by the data structure they use to implement their index, but each data structure comes with its own tradeoff between flexibility and performance.
Some data structures only support equality, whereas others support a partial order on keys as well.
In addition, if not all columns we request are covered by the index, the \acrshort{rdbms} will retrieve the locations of the rows from the index and read the columns' values for those rows from the table.
If the index covers all columns we request, the \acrshort{rdbms} directly retrieves all columns' values from the index, further improving performance.
Finally, indices also come with a performance penalty for inserting and deleting rows, as the RDBMS needs to insert or delete into both the index and the table.

The simplest data structure to back an index by is a hash table~\citep{DBLP:books/daglib/0023376}.
We call an index backed by a hash table a hash index.
A hash table is a generalized array\thinspace: rather than allowing a user to access elements directly by their index, it transforms a key into an index for a backing array by means of a hash function.
Such a hash function is a lossy conversion of a key of arbitrary type and size to an integer in amortized constant time (\(O(1)\)).
Because a hash function is not injective, collisions are possible.
A good hash function minimizes the probability of these by approximating a uniform distribution of keys over integers, but designing such a hash function is a separate science entirely \citep{DBLP:books/daglib/0023376}.

To store a key-value pair in a hash table, we hash the key to get a position in the backing array to store the value.
We note that in the worst case, we need to resize the backing array to be able to store a value, which means copying all previously inserted values for a time complexity of \(O(n)\).
To retrieve a value from a hash table, we hash the key and use the index to retrieve the value associated with it from the backing array.
We resolve collisions by applying a collision resolution algorithm such as separate chaining or open addressing \citep{DBLP:books/daglib/0023376}, both with a worst case time complexity of \(O(n)\).
An average case analysis for the insertion and retrieval operations leads to an amortized \(O(1)\) time complexity for both, as growing the backing array and resolving collisions are the exception, not the rule.

As the values in a hash table are not ordered by key, a hash index can only locate rows by equality of the hashed columns.
An alternative data structure to a hash table that supports ordering values by key is a search tree~\citep{DBLP:books/daglib/0023376}.
A search tree is a tree in which each node's left subtree contains only keys less than the key of that node and each node's right subtree only contains keys greater than the key of that node.
Thanks to this ordering, search trees allow insertion and retrieval of values in logarithmic time (\(O(\log n)\)), but this requires them to be balanced\thinspace: the size of the subtree to the left and right of every node needs to remain approximately equal to guarantee this time complexity\footnote{As an example, if we inserted elements in order into an empty search tree without balancing it along the way, it would degrade into a singly linked list and insertions and retrievals in \(O(n)\).}.
Several algorithms exist for balancing binary search trees~\citep{DBLP:books/daglib/0023376}, leading to Adelson-Velsky and Landis trees~\citep{DBLP:books/daglib/0029345} and red-black trees~\citep{DBLP:books/daglib/0023376}, among other variations.
These binary search trees derive their name from their branching factor of 2 and are the most popular search tree for ordering data in primary storage.
The branching factor for a search tree is the maximum number of nodes below any other node.

The base of the logarithm for the running time of the operations on search trees is irrelevant in algorithmic complexity theory\footnote{Changing a logarithm to another base is a division by a constant, which is asymptotically irrelevant.}, but in this case it corresponds to the branching factor of the search tree.
Because RDBMSs persist indices to secondary storage to load them into primary storage later, the branching factor of a binary search tree is too low.
A low branching factor leads to loading more nodes from secondary storage to retrieve a value and more balancing operations.
As secondary storage is an order of magnitude slower than primary storage, this impacts performance.
A B-tree~\citep{DBLP:journals/acta/BayerM72}\footnote{The inventors of the B-tree never stated what the B stands for~\citep{DBLP:journals/csur/Comer79}. Some candidates are Bayer, Boeing, balanced, bushy, and broad.} is a generalization of a red-black tree to any branching factor suitable for secondary storage.
The B\textsuperscript{+}-tree variation only stores keys in intermediate nodes and is the one implemented as the default index by most \acrshortpl{rdbms}.

\section{Time series}

A time series is a sequence of values which we index by a time parameter.
While data from panels and longitudinal studies readily fit this definition, we note that the time parameter need not represent the passing of time in the real world.
As an example of time series data with a time parameter which does not represent the passing of time in the real world, we take digital image data, where the time parameter represents the position of a pixel in a digital image.
The only property we require from our data to fit the time series definition is therefore that we index it by some parameter for which we define a partial order \citep{davey2002introduction}.
In a statistical model, we treat the values in a time series as the realizations of a random variable indexed by the same time parameter as the time series itself \citep{shumway2000time}.

In this thesis, the time series we encounter are sensor waveforms for proportional valve tests, an engineering application.
Waveforms are amenable to techniques from signal processing, such as filtering or spectral decomposition, but these techniques generalize to the statistical treatment of time series.
As an example, the Savitzky-Golay filter \citep{savitzky1964smoothing} corresponds to local polynomial regression \citep{cleveland1979robust}.
It is also possible to combine time and frequency domain analysis from signal processing, e.g. through Fourier or wavelet transforms, to time series in general \citep{shumway2000time}.

\subsection{Dimensionality reduction}

While we can treat any time series data with techniques from signal processing, we see that most waveform data share additional properties, such as the 1 kHz sample rate we see in the proportional valve test waveform data in this thesis.
It is important to be mindful of the curse of dimensionality \citep{hastie2009elements} when we apply techniques from statistics with no analog in signal processing to data with considerably more variables than observations.
Although it is possible to directly treat the 30,000 data points of each sensor waveform as the predictor variables in our regression model, as we have more proportional valves than waveform samples, it is not ideal, because the data points become sparse and the time complexity high in such a high-dimensional space.
Furthermore, we need to be particularly alert for multicollinearity in such a full model, as many of these variables are likely correlated.
In linear regression models, which fall outside the scope of this thesis, the risk of multicollinearity is complemented by a risk of autocorrelated residuals due to the time index.
Dimensionality reduction, summarizing data in a high-dimensional space in a lower-dimensional space while preserving a given characteristic of said high-dimensional data as well as possible, is in order before we solve the regression problem.
In the rest of this subsection, we give an overview of dimensionality reduction techniques.

Regularized regression aims to shrink or eliminate coefficients by penalizing their size or outright presence and therefore serves as a dimensionality reduction technique for multiple regression models.
Three variations of regularized regression are commonly used: ridge regression, \acrlong{lasso}~(\acrshort{lasso}) regression, and elastic net regression.
Ridge regression adds a regularization term which includes the Euclidean norm $L_{2}$ to the model, leading to a shrinkage of coefficients.
It does not always eliminate coefficients, they may change their sign instead.
The regularization term for \acrlong{lasso} regression includes the Manhattan distance $L_{1}$, which does eliminate coefficients.
Elastic net regression combines both regularization terms, aiming for both shrinkage and elimination of coefficients.
All three methods require hyperparameters which need tuning, for instance by constructing a cross-validation estimator.
While ridge and \acrlong{lasso} regression only require a single hyperparameter as part of their regularization term, elastic net regression requires an additional hyperparameter to determine the relative weight of its ridge and \acrshort{lasso} regularization terms.

An approach to dimensionality reduction which has been known for over a century is \acrlong{pca}~(\acrshort{pca}) \citep{doi:10.1080/14786440109462720,hotelling1933analysis}.
The aim of \acrshort{pca} is to construct uncorrelated output variables as linear combinations of input variables, called principal components (PC), which successively maximize variance \citep{doi:10.1098/rsta.2015.0202}.
When we apply \acrshort{pca}, we therefore assume linear combinations of input variables with high variance are more important than those with low variance.
This means that the principal components are not appropriate variables for use in all statistical models.
On the other hand, for us to use few principal components to faithfully represent the input variables, we require correlation between input variables, which does occur in time series data.
When we think of constructing the principal components using the variance-covariance matrix, we require a matrix inversion.
In practice, we solve an eigendecomposition problem for a square data matrix or \acrlong{svd}~\acrshort{SVD} problem for a general rectangular data matrix \citep{axler2015linear,strang1993introduction}, because their time complexity is lower.

Spectral analysis or analysis in the frequency domain is not a dimensionality reduction technique itself, but facilitates dimensionality reduction.
By converting a time series from the time domain into the frequency domain,
Three common conversions are the \acrlong{dft}~(\acrshort{dft}), the short-time DFT, and the wavelet transform.
Like \acrshort{pca}, these are changes of basis in linear algebra terms.
The \acrshort{dft} maps the time series onto a basis of sine and cosine waves.
The short-time \acrshort{dft} divides the time series into windows of a fixed length and applies the \acrshort{DFT} to these, which is useful when the frequency spectrum of the time series is known to change over time.
The wavelet transform is a generalization of the \acrshort{dft}, because it allows for a pluggable wavelet basis instead of the fixed sine and cosine basis from the \acrshort{dft}.
In addition, it allows us to trade information in the time domain for information in the frequency domain, effectively acting like a filter bank.
While it makes sense to apply \acrshort{pca} to the concatenation of multiple time series, it does not make sense to apply a spectral decomposition such as a (short time/sliding window) \acrshort{dft} or wavelet transform to it.

%PCA
%\citep{DBLP:journals/pvldb/DingTSWK08}

The piecewise linear approximation to a time series \citep{DBLP:journals/tc/PavlidisH74} approximates a time series as a sequence of line segments.
Since each piece in this approximation is a straight line, we only require a discrete start time, end time, offset, and slope variable to represent it.
As such, it is also a dimensionality reduction technique.
To compute it, we use a time series segmentation algorithm \citep{DBLP:conf/icdm/KeoghCHP01}, such as the sliding window, top-down, or bottom-up algorithm.
All of these algorithms support pluggable distance measures, such as the Euclidean norm $L_{2}$, to determine how different the segments are.
The sliding window algorithm slides a window over the data and therefore lacks a global overview, leading to subpar segmentation.
The top-down and bottom-up algorithms both operate on the whole time series, but starting from a single segment and all single value segments, respectively.

The matrix profile \citep{DBLP:conf/icdm/YehZUBDDSMK16} is the name of an algorithm which solves the similarity join problem for time series and its output data structure.
The algorithm yields the nearest neighbour to every fixed-length subsequence of a time series by some distance measure, such as $L_{2}$, which is the solution to the similarity join problem for time series.
It uses the fast Fourier transform to implement convolution operations and has some other desirable properties, such as yielding exact solutions and being embarrassingly parallelizable.
We note that the literature from the authors of the original paper is rich in neologisms such as motifs, discords, and shapelets.
Motifs are values in time series which have a near neighbour, whereas discords only have a far neighbour.
We return to shapelets in the next section.

\subsection{Classification}

Another key problem in the statistical analysis of time series data is time series classification.
This is simply the classification problem from machine learning \citep{hastie2009elements} applied to time series data, equivalent to the regression problem from traditional statistics using continuous predictor variables to predict a categorical outcome variable \citep{agresti2003categorical}.
We can therefore combine the dimensionality reduction techniques for time series data from the previous subsection with techniques for solving the classification or regression problem, yielding principal components regression \citep{hastie2009elements} and various forms of matrix profile regression \citep{DBLP:journals/frai/GuidottiD21,seoni2021template}, among other combinations.
In addition, \acrlong{lda}~(\acrshort{lda}) \citep{hastie2009elements,mardia1979multivariate} is both a technique for dimensionality reduction and a classification technique for a Gaussian mixture model.
Even when we apply \acrshort{pca} or \acrshort{lda} and use only a few of the linear combinations these yield as variables in a statistical model, the predictions from said model are hard to interpret when the number of dimensions is high.

Time series shapelets \citep{DBLP:conf/kdd/YeK09} are time series subsequences which are most representative of a class of time series in a time series classification problem.
By some distance measure, they are as close as possible to a subsequence of each time series in the class they represent and as far as possible from all subsequences of each time series in the other classes.
Even though the algorithm for finding shapelets abandons suboptimal shapelets early and prunes the search space, its time complexity is still polynomial in the amount of time series and linear in the amount of data points per time series.
For long time series, such as the sensor waveforms in the automotive supplier company's proportional valve data set, this is problematic.
Learning shapelets \citep{DBLP:conf/kdd/GrabockaSWS14} is a machine learning approach to finding time series shapelets.
It makes of the stochastic gradient descent optimization algorithm to approximate the shapelets.

\section{Software}

The first section in this chapter informs the reader about \acrshort{rdbms}, but it does not specify which \acrshort{rdbms} we plan to use.
PostgreSQL is the only free and open-source \acrshort{rdbms} with a parametric array column type, transparent compression of large columns, and dynamic analysis of query plans and query resource consumption (\texttt{EXPLAIN ANALYZE}) \citep{postgresql}.
The first two features are of use to us since we aim to store sensor waveforms.
The third feature is of use to us to find missing indices which, when added, greatly reduce time complexity of queries, especially on a considerable data set such as the proportional valve data set from the automotive supplier company.
Given these features, we choose PostgreSQL as the \acrshort{rdbms} for this thesis.

Now that we know how to store the proportional valve data set in secondary storage, let us look to our options for storing it in primary storage.
NumPy \citep{DBLP:journals/cse/WaltCV11} implements multidimensional arrays in Python.
Pandas \citep{DBLP:journals/usenix-login/Beazley12e} is a Python library for data frames built on top of NumPy which offers two features of use to us.
Firstly, it supports reading data frames directly from a PostgreSQL relational database through a \acrshort{sql} query.
Secondly, it allows us to convert a data frame from long to wide format and back.
Some statistical modelling or data visualization techniques explicitly require data to be in the long or wide format.

We mainly visualize data using matplotlib \citep{DBLP:journals/cse/Hunter07}, which works on top of NumPy.
We also use seaborn \citep{DBLP:journals/jossw/Waskom21}, which is built on top of matplotlib, but offers higher level primitives for statistical plots.
Finally, we also use Plotly, which renders plots on a graphics processing unit (GPU) via WebGL, which is useful when we need to plot many points.

Beside libraries for data visualization, we need libraries for the actual statistical analysis.
SciPy \citep{DBLP:journals/corr/abs-1907-10121} is a library for numerical mathematics which operates on NumPy arrays and is the foundation for the statistics libraries we use.
statsmodels \citep{statsmodels} is a traditional statistical modelling library which supports generalized linear models.
Scikit-learn \citep{DBLP:journals/jmlr/PedregosaVGMTGBPWDVPCBPD11} is a machine learning library which also implements hyperparameter tuning via cross-validation estimators for regression.
We use statsmodels and Scikit-learn for logistic regression on scalar variables in the fourth chapter of this master's dissertation.
Both Tslearn \citep{DBLP:journals/jmlr/TavenardFVDAHPY20} and pyts \citep{DBLP:journals/jmlr/FaouziJ20} have implementations of shapelet learning, built on top of Scikit-learn.
